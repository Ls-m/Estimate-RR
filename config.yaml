preprocessing:
  bandpass_filter:
    low_freq: 0.1  # Lower frequency for better stability
    high_freq: 0.6   # Higher frequency, well below Nyquist
    order: 4         # Lower order for better numerical stability
  use_denoiser: True  # Whether to apply denoising



training:
  model_name: "LSTMRR" # Model architecture to use options: ["LSTMRR", "RWKV", "RWKVTime", "RWKVTimeOPT"]
  batch_size: 64 # Batch size for training
  learning_rate: 5e-4 # Learning rate for the optimizer
  optimizer: "adamw" # Optimizer type options: ["adam", "adamw", "sgd"]
  weight_decay: 1e-4 # Weight decay for regularization
  scheduler: "ReduceLROnPlateau" # Learning rate scheduler type options: ["StepLR", "ReduceLROnPlateau", "CosineAnnealingLR"]
  max_epochs: 1 # Maximum number of training epochs
  num_workers: 4 # Number of workers for data loading
  criterion: "MSELoss" # Loss function options: ["MSELoss", "L1Loss"]
  checkpoint_dir: "checkpoints" # Directory to save model checkpoints
  early_stopping_patience: 10 # Patience for early stopping
  gradient_clip_val: 1.0 # Gradient clipping value
  n_freq_bins: 25 # Number of frequency bins for frequency-domain features
  pretrained_path: "pretrained_lstm_encoder.pth" # Path to save/load pretrained encoder weights
  ablation_mode: fusion # Ablation mode options: ["fusion", "time_only", "freq_only"]
  time_model_output_dim: 64
  freq_model_output_dim: 32
  use_ssl_pretraining: True  # Set to `true` to run SSL, `false` to train from scratch
  time_model_num_layers: 2
  dropout: 0.2


logging:
  log_dir: "logs" # Directory for logging
  experiment_name: "RR_Estimation" # Name of the experiment for logging
# Hardware Configuration
hardware:
  devices: 1
  precision: 32

mode:
  optuna: False


ssl:
  max_epochs: 1
  use_capnobase: False